\documentclass[12pt]{report}
\usepackage{sectsty}
\usepackage{alltt}
\usepackage{hyperref}
\renewcommand{\Large}{\fontsize{18pt}{18pt}\selectfont}
\renewcommand{\large}{\fontsize{14pt}{14pt}\selectfont}
\sectionfont{\large}
\renewcommand{\thesection}{\arabic{section}}
\begin{document}

\begin{titlepage}
\vspace*{\fill}
\begin{center}
{\Large Obstacle Detection and Avoidance Using TurtleBot Platform and XBox Kinect} \\
\vspace{18pt}
{\large Sol Boucher} \\
\vspace{14pt}
{\large Research Assistantship Report \\
	Department of Computer Science \\
	Rochester Institute of Technology} \\
\vspace{14pt}
Research Supervisor: Dr. Roxanne Canosa \\
Research Sponsor: Golisano College Honors Committee \\
\vspace{12pt}
20114/\today
\end{center}
\vspace{\fill}
Roxanne Canosa, Ph.D. \hrulefill\ Date \hrulefill
\end{titlepage}

\begin{abstract}
\thispagestyle{plain}
\setcounter{page}{2}
TODO
\end{abstract}

\setcounter{page}{3}
\tableofcontents
\clearpage
\phantomsection
\addcontentsline{toc}{section}{List of Figures}
\listoffigures
\clearpage
\phantomsection
\addcontentsline{toc}{section}{List of Tables}
\listoftables
\newpage

\section{Introduction}
TODO

\section{Similar Work (Literature Review)}
\begin{itemize}
\item{Whenever information from multiple image sensors is integrated, there is a risk that it will not line up appropriately, usually because of a simple displacement due to the sensors' relative positions or because of unique lens distortions created by the manufacturing process.  Herrera \textit{et al.} describe their noise-tolerant method for calibrating a color camera and depth camera against each other, enabling the attainment of better results than would ever be possible by calibrating the two cameras individually.  They start by computing for the color camera the two-dimensional projection coordinates in the image at which a three-dimensional point in space---the corner of a checkerboard calibration pattern---appears, then perform a distortion correction.  Next, they repeat the projection calculation for the depth image, this time using the corners of the plane on which the checkerboard rests---because the board itself isn't visible in this image---and omitting the distortion correction step, as it will be much less effective than for the color imagery.  Using the projections and data from several images with different perspectives, it is possible to calculate the rotation and translation necessary to match the two images' reference frames.  These first parameters obtained for the color camera are much better than those for the depth sensor, so the former are used to optimize the latter by performing a nonlinear error minimization; then, another minimization is performed across the parameters for both cameras until the results are convergent.  Using 35 calibration images, the authors are able to demonstrate comparable accuracy to that achieved by the proprietary calibration algorithm provided with their XBox Kinect test sensor (2011).}
\item{A common traditional method of obstacle avoidance is the potential field model, or PFM.  This model represents targets and obstacles as imaginary attractive and repulsive forces on the robot, respectively.  Stored as vectors, such forces are easily summed to find the resultant force vector, which is used directly as the robot's navigation vector.  One such implementation---the virtual force field, or VFF---uses a two-dimensional histogram grid populated from ultrasonic range sensors and holding certainty values of how likely it is that an obstacle exists at each location.  Objects of interest are assigned corresponding virtual repulsive force vectors with magnitude proportional to their certainty values and inversely proportional to their distance from the vehicle's center.  Similarly, the attractive force between the robot and its goal location is proportional to a preassigned force constant and inversely proportional it its distance from the vehicle.  After obtaining the resultant force vector, its direction and magnitude are converted into parameters usable by the drive system and issued as movement commands.  However, four major problems have been identified that effect all PFM systems, becoming increasingly noticeable as a robot moves faster:  The robot may fall into a trap situation when it reaches a dead end, a phenomenon for which workarounds exist.  The robot may also be directed in the opposite direction of its target in the case where two close objects stand in front of it with space between, a more difficult problem to handle.  Certain environments may also cause the robot to begin oscillating.  Finally, even more severe oscillations---and even collisions---occur when a robot drives down a narrow hallway with a discontinuity in its side.  Together, these factors make the same PFMs that were once seen as simple and elegant much less attractive, especially for applications relying on higher speeds (Koren and Borenstein, 1999).}
\item{One method of detecting obstacles using an RGB-D camera is to segment every plane in the point cloud and consider as obstacles both points emerging from the detected planes and planes whose surface orientations differ from that of the ground.  Surface detection may be accomplished computationally cheaply by considering pixel neighborhoods instead of performing distance searches, then computing the normal vector by finding the cross-product of two averaged vectors tangential to the local surface.  The coordinates of the points and their corresponding surface normals are transformed to Cartesian coordinates from the robot's perspective, then to spherical coordinates.  Only the plane representing the ground is considered navigable, and the RANSAC algorithm is applied to optimize the detected surfaces and compensate for noisy readings.  Each plane is then converted to its convex hull, and both horizontal planes besides the ground and planes supported by horizontal planes are considered to be navigational obstacles.  This method is able to process plane data at high speed using only sequential processing while remaining relatively accurate: the average deviation is under ten degrees, and objects are properly segmented over 90\% of the time.  The algorithm is, however, sensitive to very small objects and distant measurements (Holz \textit{et al.,} 2011).}
\item{The XBox Kinect has an infrared projector and infrared camera separated by about 7.5 cm, and a color camera about 2 cm away from the latter.  The infrared pair is able to assemble a grid of distance measurements triangulated from the lateral displacement of the projected points from the known emitter pattern.  Unfortunately, the device is unable to perform any distance measurements closer than about 0.5 m.  One method of detecting obstacles is as follows:  First, perform a voxel grid downsampling on the point cloud to decrease processing time.  Next, apply a pass-through filter to crop out regions of little interest or accuracy.  Then, use the RANSAC algorithm to perform plane detection.  Finally, Euclidean cluster extraction reveals individual obstacles, and additional analysis of those obstacles is performed in order to determine their sizes.  This procedure avoids many difficulties of using a single RGB camera, as well as enjoying faster run times than dual--RGB camera systems (Nguyen, 2012).}
\item{Another method of making use of depth information for the purpose of detecting obstacles is to examine the 3-D slopes between detected points.  The points may be considered to compose a single obstacle if this slope---measured with respect to the horizontal---is steeper than a set slope and if their height difference falls within a predetermined range.  Such obstacles may be found by searching the image from the bottom row and finding for each obstacle pixel in that row all the other pixels that meet the aforementioned criteria for that pixel.  The resulting points may also be classified as obstacle points, and the process repeated to find all such associated points.  Finally, individual objects may be picked out by applying the transitive property of the above obstacle composition criteria.  This works very well if the terrain and robot are both flat, but becomes a more difficult task as the terrain becomes rough or if the robot is expected to climb ramps (Talukder, 2002).}
\end{itemize}

\section{Background}
TODO

\section{Approach}
The point cloud coming off the Kinect was extremely dense, exhibited noticeable noise, and was consequently slow to transmit, display, and process.  Thus, the first thing I did was apply a voxel grid filter to downsample the data and eradicate most of the noise while achieving better update speeds and faster processing time.  Noticing that both Holz \textit{et al.} and Nguyen used surface detection algorithms, while Koren and Borenstein simply didn't train sensors on the floor, I also decided to apply a pass-through filter to crop the y-dimension so as to discard all points falling outside the robot's height range.  This step---which was possible because I knew the robot was going to be used chiefly in indoor environments possessing smooth terrain---meant that I, too, could ignore the floor and focus on those points that represented actual obstacles.  However, it also meant sacrificing the ability to climb ramps and traverse highly uneven floors. \\
The initial revision of my obstacle avoidance algorithm simply split the view into three parts:  The center region was used to determine whether to proceed forward or turn, the latter of which was triggered whenever the number of points in this region exceeded a set noise threshold.  Once the robot had entered a turning mode, it ceased forward motion and decided on a direction by choosing the peripheral vision field with fewer points in it.  The entire field of view was cropped in the z-dimension in order to prevent the robot from being distracted by objects well ahead of its current position. \\
The biggest problem with this initial approach was that the robot was prone to becoming stuck oscillating in place between a left and right turn when faced with a sufficiently large obstruction.  To work around this problem, I only allowed it to choose a direction of rotation as long as it wasn't already turning.  In this way, it was forced to pick a direction whenever it first encountered an obstacle, then continue turning in that direction until it was able to drive forward again.  As a side effect, it would now rotate \textit{ad infinitum} when enclosed on all sides. \\
As I continued testing, it became clear that my noise threshold was preventing the detection of small---but still significant---obstacles.  Decreasing this constant too much, however, caused the robot to turn spuriously in order to avoid offending points that were, in fact, nothing but noise.  To solve this problem, I eliminated the noise threshold altogether by instead averaging the number of points in the forward regions of the last several images taken. \\
Next, I discovered a relatively minor but undeniable problem: given a scene where the only obstacle was located mainly within one half of the center region and didn't extend into either periphery, the robot might just as easily turn toward the object as away from it, thereby forcing itself to turn farther.  Extending the peripheral regions so that each included its respective half of the center remedied this problem. \\
Because of how heavily its depth view was cropped, however, the robot was still prone to making poor decisions about which direction to turn.  In order to rectify this problem, I only cropped the forward region in the z-dimension, and began considering all points of any depth when determining which direction to turn.  This, however, resulted in the robot's being easily distracted by large objects that were far away; for instance, it would turn away from even wide hallways in search of open space.  This effect was countered by switching from choosing the direction in which to turn using the number of points in each side field of view to doing so using the average depth of the points in each of these fields.  The reader will observe a potential problem with this approach: when using the total number of points, it is difficult for a single noisy point to influence the robot's choice of direction, since it will only increase its respective direction's favorability by one unit.  In contrast, when examining only points' average distances, a single poorly-positioned noisy point could very easily skew that direction's favorability by a significant amount, especially if there aren't many points in view to begin with.  However, this is not as much of a concern as it might appear, since almost all of the Kinect's noise is confined to the far part of the z-dimension; in case the algorithm were to be used on a sensor that suffered from or conditions that favored nearer interference, an outlier filter might be added in order to eliminate voxels appearing far away from any others. \\
A trivial approach such as checking whether the area immediately ahead is clear and gauging turns by examining the number of points or their average depth, however, will never succeed in navigating more complex scenarios.  For instance, in a maze-like situation or other confined space, the robot will attempt to turn long before reaching a wall, missing the side passageway because it lacks the ability to determine that the turning corridor represents a traversable passage and turns all the way around before it ever gets there.  In order to solve this problem, it is necessary to implement the ability to distinguish between individual objects, which I did by using the Point Cloud Library's built-in implementation of the simple Euclidean cluster detection algorithm. \\
With the ability in place to segment separate objects, I set out to develop a simple algorithm capable of determining the perpendicular distances between their edges; the resultant gap measurements could then be compared to the known width of the robot to determine whether each space was traversable.  In order to find out how far apart two arbitrary objects are, however, one must first know the two component points that lie closest to each other.  As a fast and easy initial implementation, I accomplished this by averaging the x- and z-coordinates of all the points in each object: starting with these points as reference points and alternating between the two objects, I then iteratively discarded from the current object all points except those falling closer to the other object's reference point than did the current object's reference point, reassigning the point minimally distant from that same reference point on the other object as the current object's new reference point. \\
The robot was now able to discern the locations of inter-object gaps wide enough for it to fit through, but it was so adept at doing so that it even picked up those gaps lying perpendicular to the robot's view, as well as those that were behind additional obstacles.  The former problem was alleviated simply by adding a slope threshold that filtered out all gaps that were to steep to fit through.  The latter, however, required defining a tunnel-like region of constant width and height beginning at the robot's viewpoint and stretching through the gap of interest by a depth approximately equal to the robot's diameter.  With the gaps properly detected and filtered, I ranked the remaining options based on their distance from the center.  Unfortunately, it soon became clear that although this approach did a better job of finding logical paths in confined spaces, it was largely unsuitable for use with the Kinect because of the sensor's inability to detect sufficiently-close obstacles.  This meant that, before even getting through a gap, the bot would lose sight of it.  In order to work around this hardware limitation, a state machine could have been implemented and the ability to measure driving distance could have been added.  Unfortunately, such steps would have resulted in complete blindness during the time the robot was traversing the gap, and consequently a vulnerability to any unexpected environmental changes during that time.  As such, I decided to abandon the spacing detection approach and look for a more general and universally-applicable solution. \\
Toward the end of development of the gap detection algorithm, a severe problem surfaced; it was discovered that, due to a combination of distortions in the robot's coordinate system and noise, the robot was unable to detect objects as much as a couple inches high.  Noticing that all objects created prominent occlusions on the ground, I set out to try detecting these holes in the ground plane.  First, a section of the floor corresponding to the region immediately in front of the robot---and hence in its path---was selected from the rest of the point cloud by tight cropping.  Next, the slope of the floor was modeled to account for the Kinect's coordinate distortion, and all points falling outside a given height tolerance of this plane were filtered out.  The remaining floor points were then transformed and flattened such that they occupied the XZ-plane.  By examining the surface normals of the transformed sample, the edge points could be estimated.  Then, a minimum neighbors in radius outlier removal filter was applied to eliminate false positives.  The results were promising when tested on a smooth carpet: after some fine-tuning, no false positives were being detected and a good number of edge points arose when any given obstruction was placed in front of the sensor.  Unfortunately, speed had become a problem, as estimating the edge points was taking several seconds per sample.

\section{Results and Discussion}
TODO

\section{Table of Hours Worked}
TODO

\section{Conclusion}
TODO

\clearpage
\phantomsection
\addcontentsline{toc}{section}{References}
{\large References} \\
\linebreak
Herrera, Daniel C. \textit{et al.} (2011). Accurate and practical calibration of a depth and color camera pair. In \textit{Computer Vision, Image Analysis, and Processing (CAIP 2011).} June 8, 2012. $\langle$\url{http://www.ee.oulu.fi/~dherrera/papers/2011-depth_calibration.pdf}$\rangle$.\\
Holz, Dirk \textit{et al.} (2011). Real-time plane segmentation using RGB-D cameras. In \textit{Proceedings of the 15\textsuperscript{th} RoboCup International Symposium,} Istanbul, July 2011. June 11, 2012. $\langle$\url{ais.uni-bonn.de/papers/robocup2011_holz.pdf}$\rangle$.\\
Koren, Y. and Borenstein, J. (1991). Potential field methods and their inherent limitations for mobile robot navigation. In \textit{Proceedings of the IEEE Conference on Robotics and Automation,} Sacramento, April 7-12, 1991, 1398-1404. June 8, 2012. $\langle$\url{http://www-personal.umich.edu/~johannb/Papers/paper27.pdf}$\rangle$.\\
Nguyen, Van-Duc. (2012). Obstacle avoidance using the Kinect. June 15, 2012 $\langle$\url{http://scribd.com/doc/80464002/Obstacle-Avoidance-Using-the-Kinect}$\rangle$.
Talukder, A. \textit{et al.} (2002). Fast and reliable obstacle detection and segmentation for cross-country navigation. In \textit{Proceedings of the IEEE Intelligent Vehicle Symposium (IVS 02),} 610-618. June 25, 2012. $\langle$\url{http://users.soe.ucsc.edu/~manduchi/papers/IV02.pdf}$\rangle$.

\clearpage
{\Large Appendices} \\
\appendix
\section{Starting the TurtleBot}
\label{sec:start}
\begin{enumerate}
\item{Disconnect both chargers from the robot, if applicable.}
\item{Turn on the iRobot Create by pressing the power button on its back; the power light should turn green.}
\item{Unplug and remove the laptop from the TurtleBot.}
\item{Open the laptop's lid and press the power button.}
\item{Close the laptop, replace it in the chassis, and reconnect the cables.}
\item{Wait until the Ubuntu startup noise sounds; at this point, the robot is ready to accept connections.}
\item{\label{lst:connopen}From another machine, enter: \texttt{\$\ ssh turtlebot@turtlebot.rit.edu}}
\item{\label{lst:connclose}When prompted for a password, use: \texttt{turtlebot}}
\item{Once authenticated, start the robot service: \texttt{\$\ sudo service turtlebot start}}
\item{A few seconds later, the iRobot Create should beep and its power light should go out.  The robot is now ready for use.}
\item{Enter the following command to enable the Kinect: \texttt{\$\ nohup roslaunch turtlebot\_bringup kinect.launch \&}}
\item{You may now safely close your robot shell connection: \texttt{\$\ exit}}
\end{enumerate}

\section{Stopping the TurtleBot}
\begin{enumerate}
\item{Connect to the robot by following steps \ref{lst:connopen} through \ref{lst:connclose} of Appendix \ref{sec:start} on page \pageref{sec:start}}
\item{Stop the Kinect with: \texttt{\$\ kill `ps -ef | grep kinect.launch | grep -v grep | tr -s " " | cut -d " " -f 2`}}
\item{Release the iRobot Create with: \texttt{\$\ rosservice call \\ /turtlebot\_node/set\_operation\_mode 1}}
\item{Stop the robot service using: \texttt{\$\ sudo service turtlebot stop}}
\item{Shut down the robot laptop: \texttt{\$\ sudo halt}}
\item{Turn off the Create by pressing its power button.}
\item{Plug in the chargers for the iRobot Create and the laptop.}
\end{enumerate}

\section{Setting up a Development Workstation}
\begin{enumerate}
\item{Ready a machine for your use.  (We'll assumer you're using Ubuntu 10.04 through 11.10.)}
\item{Ensure that your system has either a recognized hostname or a static IP that is visible from the robot.}
\item{Add the ROS repository to your system: \texttt{\$\ sudo apt-add-repository http://packages.ros.org/ros/ubuntu}}
\item{Download the ROS package signing key: \texttt{\$\ wget \\ http://packages.ros.org/ros.key}}
\item{Add the signing key to your system: \texttt{\$\ sudo apt-key add ros.key}}
\item{Refresh your package archive cache: \texttt{\$\ sudo apt-get update}}
\item{Install the TurtleBot desktop suite: \texttt{\$\ sudo apt-get install \\ ros-electric-turtlebot-desktop}}
\item{Edit your bash configuration(\texttt{\$\ editor \~{}/.bashrc}), adding the following lines to the end:}
\begin{alltt}\begin{itemize}
\item{source /opt/ros/electric/setup.bash}
\item{export ROS\_MASTER\_URI=http://turtlebot.rit.edu:11311}
\item{export ROS\_HOSTNAME= \\ <non-NAT'd hostname or IP>}
\item{export ROS\_PACKAGE\_PATH= \\ <directory where you'll store your source code>:\$ROS\_PACKAGE\_PATH}
\end{itemize}\end{alltt}
\item{Write and close the file, then enter the following command in each of your open terminals: \texttt{\$\ source \~{}/.bashrc}}
\item{Install the Chrony NTP daemon: \texttt{\$\ sudo apt-get install chrony}}
\item{Synchronize the clock: \texttt{\$\ sudo ntpdate ntp.rit.edu}}
\end{enumerate}

\section{Upgrading to the Latest Version of PCL}
The version of the Point Cloud Library shipped with ROS lags significantly behind that available directly from the community.  These instructions show how to install the latest version of PCL on top of an existing ROS Electric installation.
\begin{enumerate}
\item{Create a folder hierarchy contain the build files and the replacement copy of the perception\_pcl stack: \texttt{\$\ mkdir -p \~{}/ros/prefix/lib}}
\item{Move into the deepest level: \texttt{\$\ cd \~{}/ros/prefix/lib}}
\item{Install the CMake configuration utility: \texttt{\$\ sudo apt-get install cmake-curses-gui}}
\item{Download the VTK library source code: \texttt{\$\ wget vtk.org/files/release/5.6/vtk-5.6.1.tar.gz}}
\item{Extract the source archive you just fetched: \texttt{\$\ tar xzvf vtk*}}
\item{Move into the extracted files' directory: \texttt{\$\ cd VTK}}
\item{Run the CMake configuration interface: \texttt{\$\ ccmake .}}
\item{In the UI, hit the \texttt{c} key, enable the \texttt{BUILD\_SHARED\_LIBS} and \texttt{VTK\_USE\_QT} parameters, then hit \texttt{c} and \texttt{q}.}
\item{Build the library: \texttt{\$\ make}}
\item{Move up one two levels into the \texttt{prefix} directory: \texttt{\$\ cd ../..}}
\item{Download the Boost library source: \texttt{\$\ wget sf.net/projects/boost/files/boost/1.46.1/boost\_1\_46\_1.tar.bz2}}
\item{Extract that source archive: \texttt{\$\ tar xjvf boost*}}
\item{Move into the new directory: \texttt{\$\ cd boost*}}
\item{Configure the build script: \texttt{\$\ ./bootstrap.sh}}
\item{Build and install the project into \texttt{/usr/local}: \texttt{\$\ sudo ./bjam install}}
\item{Move out into the top level of the folder structure you created: \texttt{\$\ cd \~{}/ros}}
\item{Install an extra ROS stack that will be required for the build: \texttt{\$\ sudo apt-get install ros-electric-qt-ros}}
\item{Install any remaining build dependencies: \texttt{\$\ rosdep install perception\_pcl}}
\item{Create a new ROS overlay in the current directory: \texttt{\$\ rosinstall . /opt/ros/electric}}
\item{Obtain a rosinstall file describing the repository for the perception stack: \texttt{\$\ roslocate info perception\_pcl $\rangle$ perception\_pcl.rosinstall}}
\item{Edit the rosinstall file to point at the correct repository for Electric: \texttt{\$\ sed -i s/unstable/electric\_unstable/ perception\_pcl.rosinstall }}
\item{Fetch the makefiles for the stack: \texttt{\$\ rosinstall . perception\_pcl.rosinstall}}
\item{Inform your shell of the overlay's location: \texttt{\$\ source setup.bash}}
\item{Move into the \texttt{cminpack} directory: \texttt{\$\ cd perception\_pcl/cminpack}}
\item{Build the cminpack package: \texttt{\$\ make}}
\item{Move into the \texttt{flann} directory: \texttt{\$\ cd ../flann}}
\item{Build the flann package: \texttt{\$\ make}}
\item{Move into the \texttt{pcl} directory: \texttt{\$\ cd ../pcl}}
\item{Build the PCL codebase: \texttt{\$\ CMAKE\_PREFIX\_PATH=\~{}/ros/prefix make}}
\item{Move into the \texttt{pcl\_ros} directory: \texttt{\$\ cd ../pcl\_ros}}
\item{Build the ROS PCL bindings: \texttt{\$\ make}}
\item{Move back out into the stack: \texttt{\$\ cd ..}}
\item{Build the stack's particulars: \texttt{\$\ make}}
\end{enumerate}

\section{ROS Commands}
This section aims to list the commands needed to interface with ROS and briefly address their commonly-used arguments.  For the sake of clarity, the following conventions are used: unless otherwise noted, arguments in $\langle$angled brackets$\rangle$ are required, while those in [square brackets] are optional.
\begin{itemize}
\item{\textbf{roscore} brings up a ROS master, which is useful for experimenting with ROS on one's workstation when the TurtleBot is not online.  However, in order to actually use this master instead of the TurtleBot's, one must do the following in each pertinent shell: \texttt{\$\ export ROS\_MASTER\_URI=http://localhost:11311}}
\item{\textbf{roscd $\langle$package$\rangle$} is a convenience script that allows one to immediately move into the root directory of the specified \textit{package}.}
\item{\textbf{roscreate-pkg $\langle$package$\rangle$ [dependencies]} initializes package directories to contain the source code for one or more modules.  The package directory structure will be created in a new subdirectory called \textit{package} within the current folder, which must appear in \texttt{\$ROS\_PACKAGE\_PATH}.  Typically, the \textit{dependencies} should include the \texttt{roscpp} package---which contains the ROS C++ bindings---as well as any other ROS packages that will be used.  The dependencies may be modified later by editing the \texttt{manifest.xml} file in the root directory of the package to add an additional \texttt{depend} tag.  ROS nodes may be added to a project by adding a \texttt{rosbuild\_add\_executable} directive to the \texttt{CMakeLists.txt} file, also located in the package root.}
\item{\textbf{rosmsg $\langle$verb$\rangle$ $\langle$arguments$\rangle$} shows information about currently-defined message types that may be passed over topics.  When \textit{verb} is \texttt{show} and the \textit{arguments} are \texttt{$\langle$package$\rangle$/$\langle$messagetype$\rangle$}, for instance, an "API reference" of the types and names of the variables in the message's corresponding struct hierarchy is displayed.}
\item{\textbf{rosparam $\langle$verb$\rangle$ $\langle$parameterpath$\rangle$} supports \textit{verb}s such as: \texttt{list}, \texttt{set}, \texttt{get}, and \texttt{delete}.  In the case of the former, the \textit{parameterpath} may be omitted if a complete listing is desired.  The \texttt{set} invocation expects an additional argument containing the new value to be appended.}
\item{\textbf{rosrun $\langle$package$\rangle$ $\langle$node$\rangle$} is simply used to execute a \textit{node} once the \textit{package} has been compiled with \texttt{make}.}
\item{\textbf{rosservice $\langle$verb$\rangle$ $\langle$servicepath$\rangle$} allows interfacing with the presently-available services over which service types may be sent.  When \textit{verb} is \texttt{list}, \textit{servicepath} may optionally be omitted, in which case all services will be shown.  With \texttt{call}, the user may call a service by passing arguments and receive a response as supported by the service type.  The \texttt{type} \textit{verb} is important, as it returns the package and service type corresponding to the service at the specified path.}
\item{\textbf{rossvc $\langle$verb$\rangle$ $\langle$arguments$\rangle$} allows querying currently-defined service types for passing over services.  When \textit{verb} is \texttt{show} and the \textit{arguments} are of the form \texttt{$\langle$package$\rangle$/$\langle$servicetype$\rangle$}, the command outputs an "API reference"--style listing of the types and names of the variables in the struct type representing the service type.}
\item{\textbf{rostopic $\langle$verb$\rangle$ $\langle$topicpath$\rangle$} provides a bridge to currently-advertised topics over which messages may be passed.  When \textit{verb} is \texttt{list}, \textit{topicpath} may optionally be omitted to list all available topics.  With \texttt{echo}, the user may subscribe to a topic and view the data that is being subscribed to it.  Conversely, invocation with \texttt{pub} allows publishing to the topic, which will influence the nodes that are presently subscribed to it.  The \texttt{type} \textit{verb} is particularly useful: it prints the package and message type of a given registered topic.}
\end{itemize}

\section{ROS Glossary}
\begin{itemize}
\item{\textbf{message.} A ROS communication packet that carries information between \textit{nodes} in a single direction.  New message types may be declared by creating text files in a package's \texttt{msg} directory and enabling the \texttt{rosbuild\_genmsg()} directive in its \texttt{CMakeLists.txt} file.  The composition of existing message types may be found using the \texttt{rosmsg show} command.  ROS automatically generates a C++ struct for each message type; these struct types are declared in the \texttt{$\langle$package$\rangle$/$\langle$messagetype$\rangle$.h} headers and must be included before they may be used.  Two examples of useful message types are \texttt{std\_msgs/Int32}---an \texttt{int}---and \texttt{geometry\_msgs/Twist}---used for driving the Create around.}
\item{\textbf{node.} A single ROS executable, which may be added to a \textit{package} by appending a \texttt{rosbuild\_add\_executable} directive to the \texttt{CMakeLists.txt} file of the latter.  Once the package has been compiled using GNU Make, each of its nodes may be run using the \texttt{rosrun} command.}
\item{\textbf{package.} A "project" containing executables and/or libraries; new packages may be created with the \texttt{roscreate-pkg} command, and existing ones may be imported into a new one by adding \texttt{depend} tags to its \texttt{manifest.xml} file.  A couple of important packages are \texttt{roscpp}, which contains the \texttt{ros/ros.h} header that allows one to interface with ROS, and \texttt{pcl\_ros}, which depends on the \texttt{pcl} package to provide the Point Cloud Library bindings.}
\item{\textbf{parameter.} A variable hosted on the ROS parameter server; it is persistent across multiple runs of a node, provided that the ROS master is not restarted.  Depending upon the node's implementation, changing one of its parameters while it is running may also affect its continued behavior.  The user interface to the parameter server is provided by the \texttt{rosparam} command, while the C++ API supports the analogous \texttt{setParam}, \texttt{getParam}, \texttt{deleteParam}, and other methods located in the \texttt{ros::NodeHandle} class.}
\item{\textbf{service.} A link between ROS \textit{nodes} allowing two-way communication carried in the form of service types from a client to a server.  The user may call an existing service using the \texttt{rosservice} command, while C++ programs may create and call services via the \texttt{ros::ServiceServer} and \texttt{ros::ServiceClient} classes, which may be built by means of the \texttt{advertiseService} and \texttt{serviceClient} methods within \texttt{ros::NodeHandle}.  Service types---the analog of \textit{messages} from the world of \textit{topics}---may be declared in text files within a package's \texttt{srv} directory after enabling its \texttt{CMakeLists.txt} file's \texttt{rosbuild\_gensrv()} call.  Service types' components may be seen with the \texttt{rosservice show} command, and C++ service structs are generated and used similarly to those for \textit{messages}.  One example of a service used on the TurtleBot is \texttt{/turtlebot\_node/set\_operation\_mode}, which takes an integer---usually 1, 2, or 3---responds whether it is valid, and brings the iRobot Create into either Passive, Safety, or Full mode.}
\item{\textbf{topic.} A link between ROS \textit{nodes} that allows one-way communication of information carried in the form of \textit{messages} from a publisher to a subscriber.  The user may publish or subscribe to a topic by means of the \texttt{rostopic} command, while C++ programs may do so by creating a \texttt{ros::Publisher} or \texttt{ros::Subscriber} object using the \texttt{ros::NodeHandle} class's \texttt{advertise} or \texttt{subscribe} method.  Examples of topics on the TurtleBot are \texttt{/vel\_cmd}---used to control the Create's drive and steering---and \texttt{/cloud\_throttled}---which publishes the point cloud from the Kinect.}
\end{itemize}

\section{Useful Links}
Unfortunately, much of the documentation for ROS (the Robot "Operating System") is rather terse and unfriendly.  Here, I've made an effort to catalog the documentation that I found most helpful.  I've also included documentation from the website for PCL (the Point Cloud Library), which is perhaps better organized and certainly more comprehensive than that available on the ROS Wiki.
\begin{itemize}
\item{ROS TurtleBot wiki: \url{http://ros.org/wiki/TurtleBot}}
\item{ROS tutorials: \url{http://ros.org/wiki/ROS/Tutorials}}
\item{ROS C++ tutorials: \url{http://ros.org/wiki/roscpp/Tutorials}}
\item{ROS C++ overview: \url{http://ros.org/wiki/roscpp/Overview}}
\item{ROS C++ API reference: \url{http://ros.org/doc/electric/api/roscpp/html}}
\item{ROS PCL data type integration examples: \url{http://ros.org/wiki/pcl_ros}}
\item{PCL tutorials: \url{http://pointclouds.org/documentation/tutorials}}
\item{PCL API reference: \url{http://docs.pointclouds.org/1.1.0}}
\end{itemize}

\end{document}
