\documentclass{article}
\usepackage{alltt}
\usepackage{hyperref}
\title{Obstacle Detection and Avoidance Using TurtleBot Platform and XBox Kinect}
\author{Sol Boucher}
\date{\today}
\begin{document}
\maketitle

\section{Summaries of related readings}
\begin{itemize}
\item{Whenever information from multiple image sensors is integrated, there is a risk that it will not line up appropriately, usually because of a simple displacement due to the sensors' relative positions or because of unique lens distortions created by the manufacturing process.  Herrera \textit{et al.} describe their noise-tolerant method for calibrating a color camera and depth camera against each other, enabling the attainment of better results than would ever be possible by calibrating the two cameras individually.  They start by computing for the color camera the two-dimensional projection coordinates in the image at which a three-dimensional point in space---the corner of a checkerboard calibration pattern---appears, then perform a distortion correction.  Next, they repeat the projection calculation for the depth image, this time using the corners of the plane on which the checkerboard rests---because the board itself isn't visible in this image---and omitting the distortion correction step, as it will be much less effective than for the color imagery.  Using the projections and data from several images with different perspectives, it is possible to calculate the rotation and translation necessary to match the two images' reference frames.  These first parameters obtained for the color camera are much better than those for the depth sensor, so the former are used to optimize the latter by performing a nonlinear error minimization; then, another minimization is performed across the parameters for \textit{both} cameras until the results are convergent.  Using 35 calibration images, the authors were able to demonstrate comparable accuracy to that achieved by the proprietary calibration algorithm provided with their XBox Kinect test sensor (2011).}
\item{A common traditional method of obstacle avoidance is the potential field model, or PFM.  This model represents targets and obstacles as imaginary attractive and repulsive forces on the robot, respectively.  Stored as vectors, such forces are easily summed to find the resultant force vector, which is used directly as the robot's navigation vector.  One such implementation---the virtual force field, or VFF---uses a two-dimensional histogram grid populated from ultrasonic range sensors and holding certainty values of how likely it is that an obstacle exists at each location.  Objects of interest are assigned corresponding virtual repulsive force vectors with magnitude proportional to their certainty values and inversely proportional to their distance from the vehicle's center.  Similarly, the attractive force between the robot and its goal location is proportional to a preassigned force constant and inverselp proportional it its distance from the vehicle.  After obtaining the resultant force vector, its direction and magnitude are converted into parameters usable by the drive system and issued as movement commands.  However, four major problems have been identified that effect all PFM systems, becoming increasingly noticable as a robot moves faster:  The robot may fall into a trap situation when it reaches a dead end, a phenomenon for which workarounds exist.  The robot may also be directed in the opposite direction of its target in the case where two close objects stand in front of it with space between, a more difficult problem to handle.  Certain environments may also cause the robot to begin oscillating.  Finally, even more severe oscillations---and even collisions---occur when a robot drives down a narrow hallway with a discontinuity in its side.  Together, these factors make the PFMs that were once thought to be simple and elegant much less attractive, especially for applications relying on higher speeds (Koren and Borenstein, 1999).}
\item{One method of detecting obstacles using an RGB-D camera is to segment every plane in the point cloud and consider as obstacles both points emerging from the detected planes and planes whose surface orientations differ from that of the ground.  Surface detection may be accomplished computationally cheaply by considering pixel neighborhoods instead of performing distance searches, then computing the normal vector by finding the cross-product of two averaged vectors tangential to the local surface.  The coordinates of the points and their corresponding surface normals are transformed to Cartesian coordinates from the robot's perspective, then to spherical coordinates.  Only the plane representing the ground is considered navigable, and the RANSAC algorithm is applied to optimize the detected surfaces and compensate for noisy readings.  Each plane is then converted to its convex hull, and both horizontal planes besides the ground and planes supported by horizontal planes are considered to be navigational obstacles.  This method is able to process plane data at high speed using only sequential processing while remaining relatively accurate: the average deviation is under ten degrees, and objects are properly segmented over 90\% of the time.  The algorithm is, however, sensitive to very small objects and distant measurements (Holz \textit{et al.,} 2011).}
\item{The XBox Kinect has an infrared projector and infrared camera separated by about 7.5 cm, and a color camera about 2 cm away from the latter.  The infrared pair is able to assemble a grid of distance measurements by triangulating the lateral displacement of the projected points from the known emitter pattern.  Unfortunately, the device is unable to perform any distance measurements closer than about 0.5 m.  One method of detecting obstacles is as follows:  First, perform a voxel grid downsampling on the point cloud to decrease processing time.  Next, apply a passthrough filter to crop out regions of little interest or accuracy.  Then, use the RANSAC algorithm to perform plane detection.  Finally, Euclidean cluster extraction reveals individual obstacles, and additional analysis of those obstacles is performed in order to determine their sizes.  This procedure avoids many difficulties of using a single RGB camera, as well as enjoying faster runtimes than dual--RGB camera systems (Nguyen, 2012).}
\end{itemize}

\section{References}
Herrera, Daniel C. \textit{et al.} (2011). Accurate and Practical Calibration of a Depth and Color Camera Pair. \textit{Computer Vision, Image Analysis, and Processing (CAIP 2011).} 8 June 2012. <\url{http://www.ee.oulu.fi/~dherrera/papers/2011-depth_calibration.pdf}>.\\
Holz, Dirk \textit{et al.} (2011). Real-Time Plane Segmentation using RGB-D Cameras. In \textit{Proceedings of the 15\textsuperscript{th} RoboCup International Symposium,} Istanbul, July 2011. 11 June 2012. <\url{ais.uni-bonn.de/papers/robocup2011_holz.pdf}>.\\
Koren, Y. and Borenstein, J. (1991). Potential Field Methods and Their Inherent Limitations for Mobile Robot Navigation. In \textit{Proceedings of the IEEE Conference on Robotics and Automation,} Sacramento, April 7-12, 1991, 1398-1404. 8 June 2012. <\url{http://www-personal.umich.edu/~johannb/Papers/paper27.pdf}>.\\
Nguyen, Van-Duc. (2012). Obstacle Avoidance using the Kinect. 15 June 2012 <\url{http://scribd.com/doc/80464002/Obstacle-Avoidance-Using-the-Kinect}>.

\section{Starting the TurtleBot}
\label{sec:start}
\begin{enumerate}
\item{Disconnect both chargers from the robot, if applicable.}
\item{Turn on the iRobot Create by pressing the power button on its back; the power light should turn green.}
\item{Unplug and remove the laptop from the TurtleBot.}
\item{Open the laptop's lid and press the power button.}
\item{Close the laptop, replace it in the chassis, and reconnect the cables.}
\item{Wait until the Ubuntu startup noise sounds; at this point, the robot is ready to accept connections.}
\item{\label{lst:connopen}From another machine, enter: \texttt{\$\ ssh turtlebot@turtlebot.rit.edu}}
\item{\label{lst:connclose}When prompted for a password, use: \texttt{turtlebot}}
\item{Once authenticated, start the robot service: \texttt{\$\ sudo service turtlebot start}}
\item{A few seconds later, the iRobot Create should beep and its power light should go out.  The robot is now ready for use.}
\end{enumerate}

\section{Stopping the TurtleBot}
\begin{enumerate}
\item{Connect to the robot by following steps \ref{lst:connopen} through \ref{lst:connclose} of section \ref{sec:start} on page \pageref{sec:start}}
\item{Stop the robot service using: \texttt{\$\ sudo service turtlebot stop}}
\item{Shut down the robot laptop: \texttt{\$\ sudo halt}}
\item{Turn off the iRobot Create by pressing its power button.}
\item{Plug in the chargers for the iRobot Create and the laptop.}
\end{enumerate}

\section{Setting up a development workstation}
\begin{enumerate}
\item{Ready a machine for your use.  (We'll assumer you're using Ubuntu 10.04 through 11.10.)}
\item{Ensure that your system has either a recognized hostname or a static IP that is visible from the robot.}
\item{Add the ROS repository to your system: \texttt{\$\ sudo apt-add-repository http://packages.ros.org/ros/ubuntu}}
\item{Download the ROS package signing key: \texttt{\$\ wget http://packages.ros.org/ros.key}}
\item{Add the signing key to your system: \texttt{\$\ sudo apt-key add ros.key}}
\item{Refresh your package archive cache: \texttt{\$\ sudo apt-get update}}
\item{Install the TurtleBot desktop suite: \texttt{\$\ sudo apt-get install ros-electric-turtlebot-desktop}}
\item{Edit your bash configuration(\texttt{\$\ editor \~{}/.bashrc}), adding the following lines to the end:}
\begin{alltt}\begin{itemize}
\item{source /opt/ros/electric/setup.bash}
\item{export ROS_MASTER_URI=http://turtlebot.rit.edu:11311}
\item{export ROS_HOSTNAME=<non-NAT'd hostname or IP>}
\item{export ROS_PACKAGE_PATH=<directory where you'll store your source code>:\$ROS_PACKAGE_PATH}
\end{itemize}\end{alltt}
\item{Write and close the file, then enter the following command in each of your open terminals: \texttt{\$\ source \~{}/.bashrc}}
\item{Install the Chrony NTP daemon: \texttt{\$\ sudo apt-get install chrony}}
\item{Synchronize the clock: \texttt{\$\ sudo ntpdate ntp.rit.edu}}
\end{enumerate}
\end{document}
