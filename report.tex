\documentclass[12pt]{report}
\usepackage{sectsty}
\usepackage{alltt}
\usepackage{hyperref}
\renewcommand{\Large}{\fontsize{18pt}{18pt}\selectfont}
\renewcommand{\large}{\fontsize{14pt}{14pt}\selectfont}
\sectionfont{\large}
\renewcommand{\thesection}{\arabic{section}}
\begin{document}

\begin{titlepage}
\vspace*{\fill}
\begin{center}
{\Large Obstacle Detection and Avoidance Using TurtleBot Platform and XBox Kinect} \\
\vspace{18pt}
{\large Sol Boucher} \\
\vspace{14pt}
{\large Research Assistantship Report \\
	Department of Computer Science \\
	Rochester Institute of Technology} \\
\vspace{14pt}
Research Supervisor: Dr. Roxanne Canosa \\
Research Sponsor: Golisano College Honors Committee \\
\vspace{12pt}
20114/\today
\end{center}
\vspace{\fill}
Roxanne Canosa, Ph.D. \hrulefill\ Date \hrulefill
\end{titlepage}

\begin{abstract}
\thispagestyle{plain}
\setcounter{page}{2}
TODO
\end{abstract}

\setcounter{page}{3}
\tableofcontents
\clearpage
\phantomsection
\addcontentsline{toc}{section}{List of Figures}
\listoffigures
\clearpage
\phantomsection
\addcontentsline{toc}{section}{List of Tables}
\listoftables
\newpage

\section{Introduction}
TODO

\section{Similar Work (Literature Review)}
\begin{itemize}
\item{Whenever information from multiple image sensors is integrated, there is a risk that it will not line up appropriately, usually because of a simple displacement due to the sensors' relative positions or because of unique lens distortions created by the manufacturing process.  Herrera \textit{et al.} describe their noise-tolerant method for calibrating a color camera and depth camera against each other, enabling the attainment of better results than would ever be possible by calibrating the two cameras individually.  They start by computing for the color camera the two-dimensional projection coordinates in the image at which a three-dimensional point in space---the corner of a checkerboard calibration pattern---appears, then perform a distortion correction.  Next, they repeat the projection calculation for the depth image, this time using the corners of the plane on which the checkerboard rests---because the board itself isn't visible in this image---and omitting the distortion correction step, as it will be much less effective than for the color imagery.  Using the projections and data from several images with different perspectives, it is possible to calculate the rotation and translation necessary to match the two images' reference frames.  These first parameters obtained for the color camera are much better than those for the depth sensor, so the former are used to optimize the latter by performing a nonlinear error minimization; then, another minimization is performed across the parameters for \textit{both} cameras until the results are convergent.  Using 35 calibration images, the authors were able to demonstrate comparable accuracy to that achieved by the proprietary calibration algorithm provided with their XBox Kinect test sensor (2011).}
\item{A common traditional method of obstacle avoidance is the potential field model, or PFM.  This model represents targets and obstacles as imaginary attractive and repulsive forces on the robot, respectively.  Stored as vectors, such forces are easily summed to find the resultant force vector, which is used directly as the robot's navigation vector.  One such implementation---the virtual force field, or VFF---uses a two-dimensional histogram grid populated from ultrasonic range sensors and holding certainty values of how likely it is that an obstacle exists at each location.  Objects of interest are assigned corresponding virtual repulsive force vectors with magnitude proportional to their certainty values and inversely proportional to their distance from the vehicle's center.  Similarly, the attractive force between the robot and its goal location is proportional to a preassigned force constant and inversely proportional it its distance from the vehicle.  After obtaining the resultant force vector, its direction and magnitude are converted into parameters usable by the drive system and issued as movement commands.  However, four major problems have been identified that effect all PFM systems, becoming increasingly noticeable as a robot moves faster:  The robot may fall into a trap situation when it reaches a dead end, a phenomenon for which workarounds exist.  The robot may also be directed in the opposite direction of its target in the case where two close objects stand in front of it with space between, a more difficult problem to handle.  Certain environments may also cause the robot to begin oscillating.  Finally, even more severe oscillations---and even collisions---occur when a robot drives down a narrow hallway with a discontinuity in its side.  Together, these factors make the PFMs that were once thought to be simple and elegant much less attractive, especially for applications relying on higher speeds (Koren and Borenstein, 1999).}
\item{One method of detecting obstacles using an RGB-D camera is to segment every plane in the point cloud and consider as obstacles both points emerging from the detected planes and planes whose surface orientations differ from that of the ground.  Surface detection may be accomplished computationally cheaply by considering pixel neighborhoods instead of performing distance searches, then computing the normal vector by finding the cross-product of two averaged vectors tangential to the local surface.  The coordinates of the points and their corresponding surface normals are transformed to Cartesian coordinates from the robot's perspective, then to spherical coordinates.  Only the plane representing the ground is considered navigable, and the RANSAC algorithm is applied to optimize the detected surfaces and compensate for noisy readings.  Each plane is then converted to its convex hull, and both horizontal planes besides the ground and planes supported by horizontal planes are considered to be navigational obstacles.  This method is able to process plane data at high speed using only sequential processing while remaining relatively accurate: the average deviation is under ten degrees, and objects are properly segmented over 90\% of the time.  The algorithm is, however, sensitive to very small objects and distant measurements (Holz \textit{et al.,} 2011).}
\item{The XBox Kinect has an infrared projector and infrared camera separated by about 7.5 cm, and a color camera about 2 cm away from the latter.  The infrared pair is able to assemble a grid of distance measurements by triangulating the lateral displacement of the projected points from the known emitter pattern.  Unfortunately, the device is unable to perform any distance measurements closer than about 0.5 m.  One method of detecting obstacles is as follows:  First, perform a voxel grid downsampling on the point cloud to decrease processing time.  Next, apply a pass-through filter to crop out regions of little interest or accuracy.  Then, use the RANSAC algorithm to perform plane detection.  Finally, Euclidean cluster extraction reveals individual obstacles, and additional analysis of those obstacles is performed in order to determine their sizes.  This procedure avoids many difficulties of using a single RGB camera, as well as enjoying faster run times than dual--RGB camera systems (Nguyen, 2012).}
\end{itemize}

\section{Background}
TODO

\section{Approach}
The point cloud coming off the Kinect exhibited noticeable noise, was extremely dense, and was consequently slow to transmit, display, and process.  Consequently, the first thing I did was apply a voxel grid filter to downsample the data and eradicate most of the noise while achieving better update speeds and faster processing time.  Noticing that both Holz \textit{et al.} and Nguyen used surface detection algorithms, while Koren and Borenstein simply didn't train sensors on the floor, I also decided to apply a pass-through filter to crop the y-dimension so as to discard all points falling outside the robot's height range.  This step---which was possible because I knew the robot was going to be used chiefly in indoor environments possessing smooth terrain---meant that I, too, could ignore the floor and focus on those points that represented actual obstacles.  However, it also meant sacrificing the ability to climb ramps and traverse highly uneven floors.
The initial revision of my obstacle avoidance algorithm simply split the view into three parts:  The center region was used to determine whether to proceed forward or turn, the latter of which was triggered whenever the number of points in this region exceeded a set noise threshold.  Once the robot had entered a turning mode, it ceased forward motion and decided on a direction by choosing the peripheral vision field with fewer points in it.  The entire field of view was cropped in the z-dimension in order to prevent the robot from being distracted by objects well ahead of its current position.  The biggest problem with this initial revision was that the robot was prone to become stuck in place oscillating between a left and right turn when faced with a sufficiently large obstruction.  To work around this problem, I only allowed it to choose a direction of rotation as long as it wasn't already turning.  In this way, it was forced to pick a direction whenever it first encountered an obstacle, then continue turning in that direction until it was able to drive forward again.  Consequentially, it would now rotate \textit{ad infinitum} when enclosed on all sides.
As I continued testing, it became clear that my noise threshold was preventing the detection of small---but still significant---obstacles.  Decreasing this constant too much, however, caused the robot to turn spuriously in order to avoid offending points that were, in fact, nothing but noise.  To solve this problem, I eliminated the noise threshold altogether by instead averaging the number of points in the forward regions of the last several images taken.
Next, I discovered a relatively minor but undeniable problem: given a scene where the only obstacle was located mainly within one half of the center region and didn't extend into either periphery, the robot might just as easily turn toward the object as away from it, thereby forcing itself to turn farther.  Extending the peripheral regions so that each included its respective half of the center remedied this problem.
Because of how heavily its depth view was cropped, the robot was prone to making poor decisions about which direction to turn.  In order to rectify this problem, I only cropped the forward region in the z-dimension, and began considering all points of any depth when determining which direction to turn.  This, however, resulted in the robot's being easily distracted by large objects that were far away; for instance, it would turn away from even wide hallways in search of open space.  This effect was countered by switching from choosing a turn direction using the number of points in each side field of view to doing so using the average depth of the points in each field.  The reader will observe a potential problem with this approach: when using the total number of points, it is difficult for a single noisy point to influence the robot's choice of direction, since it will only increase its respective direction's favorability by one unit.  In contrast, when examining only points' average distances, a single poorly-positioned noisy point could very easily skew its direction's favorability by a significant amount, especially if there aren't many points in view to begin with.  However, this is not as much of a concern as it might appear, since almost all of the Kinect's noise is confined to the far part of the z-dimension; in case the algorithm were to be used on a sensor that suffered from nearer interference, an outlier filter might be used to eliminate voxels appearing far away from any others.

\section{Results and Discussion}
TODO

\section{Table of Hours Worked}
TODO

\section{Conclusion}
TODO

\clearpage
\phantomsection
\addcontentsline{toc}{section}{References}
{\large References} \\
\linebreak
Herrera, Daniel C. \textit{et al.} (2011). Accurate and Practical Calibration of a Depth and Color Camera Pair. \textit{Computer Vision, Image Analysis, and Processing (CAIP 2011).} 8 June 2012. <\url{http://www.ee.oulu.fi/~dherrera/papers/2011-depth_calibration.pdf}>.\\
Holz, Dirk \textit{et al.} (2011). Real-Time Plane Segmentation using RGB-D Cameras. In \textit{Proceedings of the 15\textsuperscript{th} RoboCup International Symposium,} Istanbul, July 2011. 11 June 2012. <\url{ais.uni-bonn.de/papers/robocup2011_holz.pdf}>.\\
Koren, Y. and Borenstein, J. (1991). Potential Field Methods and Their Inherent Limitations for Mobile Robot Navigation. In \textit{Proceedings of the IEEE Conference on Robotics and Automation,} Sacramento, April 7-12, 1991, 1398-1404. 8 June 2012. <\url{http://www-personal.umich.edu/~johannb/Papers/paper27.pdf}>.\\
Nguyen, Van-Duc. (2012). Obstacle Avoidance using the Kinect. 15 June 2012 <\url{http://scribd.com/doc/80464002/Obstacle-Avoidance-Using-the-Kinect}>.

\clearpage
{\Large Appendices} \\
\appendix
\section{Starting the TurtleBot}
\label{sec:start}
\begin{enumerate}
\item{Disconnect both chargers from the robot, if applicable.}
\item{Turn on the iRobot Create by pressing the power button on its back; the power light should turn green.}
\item{Unplug and remove the laptop from the TurtleBot.}
\item{Open the laptop's lid and press the power button.}
\item{Close the laptop, replace it in the chassis, and reconnect the cables.}
\item{Wait until the Ubuntu startup noise sounds; at this point, the robot is ready to accept connections.}
\item{\label{lst:connopen}From another machine, enter: \texttt{\$\ ssh turtlebot@turtlebot.rit.edu}}
\item{\label{lst:connclose}When prompted for a password, use: \texttt{turtlebot}}
\item{Once authenticated, start the robot service: \texttt{\$\ sudo service turtlebot start}}
\item{A few seconds later, the iRobot Create should beep and its power light should go out.  The robot is now ready for use.}
\item{Enter the following command to enable the Kinect: \texttt{\$\ nohup roslaunch turtlebot\_bringup kinect.launch \&}}
\item{You may now safely close your robot shell connection: \texttt{\$\ exit}}
\end{enumerate}

\section{Stopping the TurtleBot}
\begin{enumerate}
\item{Connect to the robot by following steps \ref{lst:connopen} through \ref{lst:connclose} of Appendix \ref{sec:start} on page \pageref{sec:start}}
\item{Stop the Kinect with: \texttt{\$\ kill `ps -ef | grep kinect.launch | grep -v grep | tr -s " " | cut -d " " -f 2`}}
\item{Release the iRobot Create with: \texttt{\$\ rosservice call \\ /turtlebot\_node/set\_operation\_mode 1}}
\item{Stop the robot service using: \texttt{\$\ sudo service turtlebot stop}}
\item{Shut down the robot laptop: \texttt{\$\ sudo halt}}
\item{Turn off the Create by pressing its power button.}
\item{Plug in the chargers for the iRobot Create and the laptop.}
\end{enumerate}

\section{Setting up a development workstation}
\begin{enumerate}
\item{Ready a machine for your use.  (We'll assumer you're using Ubuntu 10.04 through 11.10.)}
\item{Ensure that your system has either a recognized hostname or a static IP that is visible from the robot.}
\item{Add the ROS repository to your system: \texttt{\$\ sudo apt-add-repository http://packages.ros.org/ros/ubuntu}}
\item{Download the ROS package signing key: \texttt{\$\ wget \\ http://packages.ros.org/ros.key}}
\item{Add the signing key to your system: \texttt{\$\ sudo apt-key add ros.key}}
\item{Refresh your package archive cache: \texttt{\$\ sudo apt-get update}}
\item{Install the TurtleBot desktop suite: \texttt{\$\ sudo apt-get install \\ ros-electric-turtlebot-desktop}}
\item{Edit your bash configuration(\texttt{\$\ editor \~{}/.bashrc}), adding the following lines to the end:}
\begin{alltt}\begin{itemize}
\item{source /opt/ros/electric/setup.bash}
\item{export ROS\_MASTER\_URI=http://turtlebot.rit.edu:11311}
\item{export ROS\_HOSTNAME= \\ <non-NAT'd hostname or IP>}
\item{export ROS\_PACKAGE\_PATH= \\ <directory where you'll store your source code>:\$ROS\_PACKAGE\_PATH}
\end{itemize}\end{alltt}
\item{Write and close the file, then enter the following command in each of your open terminals: \texttt{\$\ source \~{}/.bashrc}}
\item{Install the Chrony NTP daemon: \texttt{\$\ sudo apt-get install chrony}}
\item{Synchronize the clock: \texttt{\$\ sudo ntpdate ntp.rit.edu}}
\end{enumerate}

\section{Useful links}
Unfortunately, much of ROS (the Robot "Operating System") is rather terse and unfriendly.  Here, I've made an effort to document the documentation that I found most helpful.  I've also included documentation from the website for PCL (the Point Cloud Library), which is perhaps better organized and certainly more comprehensive than that available on the ROS Wiki.
\begin{itemize}
\item{\href{http://ros.org/wiki/Robots/TurtleBot}{ROS TurtleBot wiki}}
\item{\href{http://ros.org/wiki/ROS/Tutorials}{ROS tutorials}}
\item{\href{http://ros.org/wiki/roscpp/Tutorials}{ROS C++ tutorials}}
\item{\href{http://ros.org/wiki/roscpp/Overview}{ROS C++ overview}}
\item{\href{http://ros.org/doc/electric/api/roscpp/html}{ROS C++ API reference}}
\item{\href{http://ros.org/wiki/pcl_ros}{ROS PCL data type integration examples}}
\item{\href{http://pointclouds.org/documentation/tutorials}{PCL tutorials}}
\item{\href{http://docs.pointclouds.org/1.1.0}{PCL API reference}}
\end{itemize}

\end{document}
